{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder in Keras\n",
    "\n",
    "[VAE Recap: Variational Lower Bound](#VAE-Recap:-Variational-Lower-Bound)\n",
    "\n",
    "[VAE Recap: Structure](#VAE-Recap:-Structure)\n",
    "\n",
    "[VAE in TensorFlow](#VAE-in-TensorFlow)\n",
    "\n",
    "[VAR in Keras](#VAE-in-Keras)\n",
    "\n",
    "- [Load MNIST Data](#Load-MNIST-Data)\n",
    "\n",
    "- [Build VAE Structure](#Build-VAE-Structure)\n",
    "\n",
    "- [Define or Restore Models](#Define-or-Restore-Models)\n",
    "\n",
    "- [Visualization: Latent Space](#Visualization:-Latent-Space)\n",
    "\n",
    "- [Visualization: 2D manifold of the digits](#Visualization:-2D-manifold-of-the-digits)\n",
    "\n",
    "- [Visualization: Data Imputation](#Visualization:-Data-Imputation)\n",
    "\n",
    "[Reparameterization Trick](#Reparameterization-Trick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Recap: Variational Lower Bound\n",
    "\n",
    "### Derive ELBO\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fig/ELBO.png\" alt=\"\" width=\"600\">\n",
    "</figure>\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "    \\log p_{\\mathbf{\\theta}}(X)\n",
    "    &=\\log\\int_{Z} p_{\\theta}(X,Z)\\\\\n",
    "    &=\\log\\int_{Z} \\frac{{q_{\\phi}(Z)}}{{q_{\\phi}(Z)}}{p_{\\theta}(X,Z)}\n",
    "     =\\log\\int_{Z} {q_{\\phi}(Z)}\\frac{{p_{\\theta}(X,Z)}}{{q_{\\phi}(Z)}}\\\\\n",
    "    &={\\log\\mathbb{E}_{q_{\\phi}(Z)}\\frac{p_{\\theta}(X,Z)}{q_{\\phi}(Z)}} \n",
    "    {\\geq\\mathbb{E}_{q_{\\phi}(Z)}\\log\\frac{p_{\\theta}(X,Z)}{q_{\\phi}(Z)}}\\\\\n",
    "    &\\overset{\\Delta}{=}\\mathcal{L}(\\theta,q)\n",
    "\\end{align*} -->\n",
    "\n",
    "$\\mathcal{L}(\\theta,q)$ is referred as Variational Lower Bound, {\\it a.k.a.} ELBO.\n",
    "\n",
    "### Calculate GAP\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fig/gap.png\" alt=\"\" width=\"600\">\n",
    "</figure>\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "  &\\log p_{\\theta}(X)-\\mathcal{L}(\\theta,q)\\\\\n",
    "  =&\\log p_{\\theta}(X)\n",
    "  -\\mathbb{E}_{q_{\\phi}(Z)}\\log\\frac{p_{\\theta}(X,Z)}{q_{\\phi}(Z)}\\\\\n",
    "  =&\\mathbb{E}_{q_{\\phi}(Z)}\\left[\n",
    "  \\log p_{\\theta}(X)-\\log\\frac{p_{\\theta}(X,Z)}{q_{\\phi}(Z)}\\right]\\\\\n",
    "  =&\\mathbb{E}_{q_{\\phi}(Z)}\\left\n",
    "  [\\log\\frac{p_{\\theta}(X)}{p_{\\theta}(X,Z)}\\cdot q_{\\phi}(Z)\\right]\\\\\n",
    "  =&\\mathbb{E}_{q_{\\phi}(Z)}\\left\n",
    "  [\\log\\frac{q_{\\phi}(Z)}{p_{\\theta}(Z\\mid X)}\\right]\\\\\n",
    "  =&{\\mathcal{KL}(q_{\\phi}(Z)\\parallel p_{\\theta}(Z\\mid X))}\n",
    "\\end{align*} -->\n",
    "\n",
    "### Understand GAP\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fig/gapfig.png\" alt=\"\" width=\"400\">\n",
    "</figure>\n",
    "\n",
    "### Calculate ELBO\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fig/revisitELBO.png\" alt=\"\" width=\"600\">\n",
    "</figure> \n",
    "\n",
    "#### Our GOAL is to MAXIMIZE $\\mathcal{L}(\\theta, q)$ (variational lower bound). i.e.  MINIMIZE $-\\mathcal{L}(\\theta, q)$\n",
    "Let's calculate each of the two terms.\n",
    "\n",
    "### Regularization (KL divergence)\n",
    "*Ref: [the derivation of KL divergence of two Guassians](https://zhuanlan.zhihu.com/p/22464760)*\n",
    "<figure>\n",
    "  <img src=\"fig/klp1p2.svg\" alt=\"\" width=\"550\">\n",
    "</figure>\n",
    "\n",
    "Common assumptions\n",
    "- use $\\mathcal{N}(0,1)$ as prior for $p(Z)$\n",
    "- $q(Z\\mid X)$ is Gaussian with parameters $(\\mu(X),\\sigma(X))$ determined by Neural Networks\n",
    "\n",
    "This can be calculated analytically:\n",
    "\\begin{align*}\n",
    "-\\mathcal{KL}(q_{\\phi}(Z\\mid X)\\parallel p_{\\theta}(Z\\mid X))\n",
    "=\\frac{1}{2} \\sum_{i=1}^N \\sum_{s=1}^D\n",
    "\\left(1+\\log(\\sigma_s^{{(i)}^2})\n",
    "-\\mu_s^{{(i)}^2}\n",
    "-\\sigma_s^{{(i)}^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "### Reconstruction (Croos entropy & L2 Loss) \n",
    "$$\\mathbb{E}_{q_{\\phi}(Z\\mid X)}\\log{p_{\\theta}(X \\mid Z)}$$\n",
    "##### Bernoulli $X\\sim Ber(Z)$ (cross entropy) \n",
    "\\begin{equation}  \n",
    "\\left\\{  \n",
    "     \\begin{array}{lr}  \n",
    "     p(X=1\\mid Z) = DecoderNeuralNets(Z) = \\hat{X}\\\\\n",
    "     p(X=0\\mid Z) = 1- DecoderNeuralNets(Z)= 1-\\hat{X}\n",
    "     \\end{array}  \n",
    "\\right.  \n",
    "\\end{equation}  \n",
    "\n",
    "$$p(X\\mid Z) = \\hat{X}^X \\cdot (1-\\hat{X})^{1-X}$$\n",
    "$$\\log p(X\\mid Z) = X \\cdot log(\\hat{X}) + (1-X) \\log(1-\\hat{X})=-H(X,\\hat{X})$$\n",
    "\n",
    "$$max \\; -H(X,\\hat{X}) = min \\; H(X,\\hat{X})$$\n",
    "$H$ refers to the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) in information theory.\n",
    "\n",
    "##### Continuous $X\\sim \\mathcal{N}(Z)$ (L2 Loss) \n",
    "$$p(X\\mid Z)=\\frac{1}{\\sqrt{2\\pi}\\sigma} \\cdot e^{-\\frac{(X-\\mu)^2}{2\\sigma^2}}$$\n",
    "$$\\log p(X\\mid Z) = -\\frac{\\log(2\\pi)\\sigma^2}{2} -\\frac{(X-\\mu)^2}{2\\sigma^2}$$\n",
    "$$\\max -\\frac{(X-\\mu)^2}{2\\sigma^2} = \\min \\; (X-\\mu)^2$$\n",
    "\n",
    "## VAE Recap: Structure\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fig/vaestr1.png\" alt=\"\" width=\"700\">\n",
    "</figure>\n",
    "\n",
    "More specifically, if you build a VAE based on MLP (multi-layer perceptron), the structure looks like\n",
    "<figure>\n",
    "  <img src=\"fig/vaestr2.png\" alt=\"\" width=\"700\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% vanillia VAE \n",
    "\n",
    "class VAE(object):\n",
    "    def __init__(self, x, problem, input_dim, hidden_encoder_dim, hidden_decoder_dim, latent_dim, \n",
    "                 keep_prob, BN=True, training = None, epsl = 1e-3, lam = 0):\n",
    "\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "        b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "        self.l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "        \n",
    "        # Hidden layer encoder\n",
    "        hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "        \n",
    "        W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "        b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "        self.l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "        \n",
    "        # Mu encoder\n",
    "        mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "        \n",
    "        W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "        b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "        self.l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "        \n",
    "        # Sigma encoder\n",
    "        logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "        \n",
    "        # Sample latent variable\n",
    "        epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "        \n",
    "        std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "        self.z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "        \n",
    "        W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "        b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "        self.l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "        \n",
    "        # Hidden layer decoder\n",
    "        hidden_decoder = tf.nn.relu(tf.matmul(self.z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "\n",
    "        W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "        b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "        self.l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "        \n",
    "        KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2)\\\n",
    "                                   - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "        \n",
    "        self.pred = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) \\\n",
    "                    + b_decoder_hidden_reconstruction\n",
    "        if problem == \"classification\":\n",
    "            BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.pred, labels=self.x_weighted), reduction_indices=1)\n",
    "        if problem == \"regression\":\n",
    "            BCE = tf.sqrt(tf.nn.l2_loss(self.pred-x)*2)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(BCE + KLD)\n",
    "        \n",
    "        self.regularized_loss = self.loss + lam * self.l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE in Keras\n",
    "\n",
    "### Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: Keras implementation of a variational autoencoder\n",
    "\n",
    "    1. This VAE is built with MLP (multi-layer perceptron) and tested on MNIST.\n",
    "    1. You can save trained models.\n",
    "    2. You can restore trained models to use directly, or train additional epochs.\n",
    "    3. Visualization includes: latent space, 2D manifold, imputation.\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model, load_model\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "import tensorflow as tf\n",
    "\n",
    "#%% Load data \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#or manually download from: http://yann.lecun.com/exdb/mnist/\n",
    "mnist = input_data.read_data_sets('MNIST', one_hot=False)\n",
    "x_train = mnist.train.images\n",
    "x_test  = mnist.test.images\n",
    "y_test  = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build VAE Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify params\n",
    "np.random.seed(0)  #for reproducibility\n",
    "            \n",
    "dim_x       = 784\n",
    "dim_latent  = 2 \n",
    "dim_hidden  = 256\n",
    "batch_size  = 100 \n",
    "epochs      = 5\n",
    "decay       = 1e-4 # L2 regularization\n",
    "epsilon_std = 1.0\n",
    "use_loss    = 'xent' # 'mse' or 'xent' = - E_q(log(p)) = - \\sum_x q(x) * log(p(x))\n",
    "use_bias    = True\n",
    "\n",
    "## Encoder\n",
    "x = Input(batch_shape=(batch_size, dim_x))\n",
    "\n",
    "h_encoded = Dense(dim_hidden, \n",
    "                  kernel_regularizer=l2(decay), bias_regularizer=l2(decay), \n",
    "                  use_bias=use_bias, activation='tanh')(x)\n",
    "z_mean    = Dense(dim_latent, \n",
    "                  kernel_regularizer=l2(decay), bias_regularizer=l2(decay), \n",
    "                  use_bias=use_bias)(h_encoded)\n",
    "#why do we encode \"log_var\" instead of \"var\"?  <3*> variance is positive\n",
    "z_log_var = Dense(dim_latent, \n",
    "                  kernel_regularizer=l2(decay), bias_regularizer=l2(decay), \n",
    "                  use_bias=use_bias)(h_encoded)\n",
    "\n",
    "## Sampler\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal_variable(shape=(z_mean.get_shape()), mean=0.,\n",
    "                                       scale=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(dim_latent,))([z_mean, z_log_var])\n",
    "\n",
    "## Decoder\n",
    "decoder_hidden = Dense(dim_hidden, \n",
    "                       kernel_regularizer=l2(decay), bias_regularizer=l2(decay), \n",
    "                       use_bias=use_bias, activation='tanh')\n",
    "decoder_output = Dense(dim_x, \n",
    "                       kernel_regularizer=l2(decay), bias_regularizer=l2(decay), \n",
    "                       use_bias=use_bias, activation='sigmoid')\n",
    "x_hat          = decoder_output(decoder_hidden(z))\n",
    "\n",
    "## Loss\n",
    "def loss(x, x_hat):\n",
    "    loss_kl   = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    loss_xent = dim_x * objectives.binary_crossentropy(x, x_hat)\n",
    "    loss_mse  = dim_x * objectives.mse(x, x_hat) \n",
    "    if use_loss == 'xent':\n",
    "        return loss_kl + loss_xent\n",
    "    elif use_loss == 'mse':\n",
    "        return loss_kl + loss_mse\n",
    "    else:\n",
    "        raise Exception('Undefined Loss: %s'%(use_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define or Restore Models\n",
    "\n",
    "Reference of [save and restore in Keras](https://morvanzhou.github.io/tutorials/machine-learning/keras/3-1-save/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 11s - loss: 201.9542 - val_loss: 186.9852\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 16s - loss: 182.8704 - val_loss: 179.0592\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 17s - loss: 179.3053 - val_loss: 176.5910\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 16s - loss: 176.0410 - val_loss: 173.1587\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 16s - loss: 172.9785 - val_loss: 171.6866\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (100, 784)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (100, 256)            200960                                       \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (100, 2)              514                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (100, 2)              514                                          \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (100, 2)              0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  multiple              768                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  multiple              201488                                       \n",
      "====================================================================================================\n",
      "Total params: 404,244.0\n",
      "Trainable params: 404,244.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check if there exists previously trained models\n",
    "savepath = \"./save/\"\n",
    "if not os.path.exists(savepath): os.mkdir(savepath)\n",
    "if os.path.exists(savepath+\"vae.h5\"): ## Load trained models\n",
    "    vae       = load_model(savepath+\"vae.h5\", custom_objects = {'loss': loss, 'epsilon_std': epsilon_std})\n",
    "    encoder   = load_model(savepath+\"encoder.h5\")\n",
    "    generator = load_model(savepath+\"generator.h5\", custom_objects = {'epsilon_std': epsilon_std})\n",
    "    print(\"\\nModels trained for %d epochs successfully restored.\"%(np.load(savepath+\"epochs.npy\")))\n",
    "    \n",
    "else: ## Train a new model\n",
    "    vae = Model(x, x_hat)\n",
    "    vae.compile(optimizer='rmsprop', loss=loss)\n",
    "    ## train the VAE on MNIST digits\n",
    "    vae.fit(x_train, x_train,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, x_test))    \n",
    "    vae.save(savepath+\"vae.h5\")\n",
    "    \n",
    "    ## build a model to project inputs on the latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "    encoder.save(savepath+\"encoder.h5\")\n",
    "    \n",
    "    ## build a digit generator that can sample from the learned distribution\n",
    "    z_sampled = Input(shape=(dim_latent,))\n",
    "    x_decoded = decoder_output(decoder_hidden(z_sampled))\n",
    "    generator = Model(z_sampled, x_decoded)\n",
    "    generator.save(savepath+\"generator.h5\")\n",
    "    \n",
    "    ## document training info\n",
    "    np.save(savepath+\"epochs\", epochs)\n",
    "\n",
    "# you can summarize your model if necessary\n",
    "vae.summary()\n",
    "\n",
    "# Please kindly ignore the warning:\n",
    "#   UserWarning: No training configuration found in save file: \n",
    "#   the model was *not* compiled. Compile it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae       = load_model(savepath+\"vae.h5\", custom_objects = {'loss': loss, 'epsilon_std': epsilon_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-beffc0b26d7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# display a 2D plot of the digit classes in the latent space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_test_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "fig.savefig(savepath+\"z_{}.png\".format(use_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: 2D manifold of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5cd788b3c0fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mx_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mdigit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_decoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         figure[i * m: (i + 1) * m,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "n = 15  # figure with 15x15 digits\n",
    "m = 28\n",
    "figure = np.zeros((m * n, m * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through \n",
    "# the inverse CDF (ppf) of the Gaussian to produce values of the latent \n",
    "# variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(m, m)\n",
    "        figure[i * m: (i + 1) * m,\n",
    "               j * m: (j + 1) * m] = digit\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()\n",
    "fig.savefig(savepath+\"x_{}.png\".format(use_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAABkCAYAAAB+brwSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noCBgKMJDgQIFChQEjP8Lx+BWHOXPNFYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02a4b01f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = np.zeros((m * 3, m * n))\n",
    "x_origin = x_test[:batch_size,:]\n",
    "x_corrupted = np.copy(x_origin)\n",
    "x_corrupted[:, 300:400] = 0\n",
    "x_reconstructed = vae.predict(x_corrupted, batch_size=batch_size).reshape((-1, m, m))\n",
    "x_origin = x_origin.reshape((-1, m, m))\n",
    "x_corrupted = x_corrupted.reshape((-1, m, m))\n",
    "for i in range(n):\n",
    "    figure[:m,      i*m:(i+1)*m] = x_origin[i]\n",
    "    figure[ m:2*m,  i*m:(i+1)*m] = x_corrupted[i]\n",
    "    figure[   2*m:, i*m:(i+1)*m] = x_reconstructed[i]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.title('Image Imputation')\n",
    "plt.xticks([])\n",
    "plt.yticks(m*np.array([.5,1.5,2.5]),['Original','Corrupt','Re-con'])\n",
    "fig.savefig(savepath+\"i_{}.png\".format(use_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparameterization Trick\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fig/reparam-pic.png\" alt=\"\" width=\"600\">\n",
    "</figure>\n",
    "\n",
    "*Further readings:*\n",
    "\n",
    "*[1] [A very simple implementation of  Reparameterisation Tricks](http://nbviewer.jupyter.org/github/gokererdogan/Notebooks/blob/master/Reparameterization%20Trick.ipynb)*\n",
    "\n",
    "*[2] [Machine Learning Trick of the Day (4): Reparameterisation Tricks](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/)*\n",
    "\n",
    "*[3] [Machine Learning Trick of the Day (5): Log Derivative Trick](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/)*\n",
    "\n",
    "*[4] [Machine Learning: A Probabilistic Perspective.](http://dsd.future-lab.cn/members/2015nlp/Machine_Learning.pdf) Ch. 21*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
